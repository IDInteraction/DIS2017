\documentclass{sigchi-ext}
% Please be sure that you have the dependencies (i.e., additional
% LaTeX packages) to compile this example.
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[scaled=.92]{helvet} % for proper fonts
\usepackage{graphicx} % for EPS use the graphics package instead
\usepackage{balance}  % for useful for balancing the last columns
\usepackage{booktabs} % for pretty table rules
\usepackage{ccicons}  % for Creative Commons citation icons
\usepackage{ragged2e} % for tighter hyphenation

% Some optional stuff you might like/need.
% \usepackage{marginnote} 
% \usepackage[shortlabels]{enumitem}
% \usepackage{paralist}
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
% \copyrightinfo{Permission to make digital or hard copies of all or
% part of this work for personal or classroom use is granted without
% fee provided that copies are not made or distributed for profit or
% commercial advantage and that copies bear this notice and the full
% citation on the first page. Copyrights for components of this work
% owned by others than ACM must be honored. Abstracting with credit is
% permitted. To copy otherwise, or republish, to post on servers or to
% redistribute to lists, requires prior specific permission and/or a
% fee. Request permissions from permissions@acm.org.\\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{SIGCHI Extended Abstracts Sample File: Note Initial
  Caps} \def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Behavioural coding; object tracking; visual attention; reproducible methods}
\def\plaingeneralterms{Documentation, Standardization}

\title{How should we train a tool to recognise behaviour?}

\numberofauthors{6}
% Notice how author names are alternately typesetted to appear ordered
% in 2-column format; i.e., the first 4 autors on the first column and
% the other 4 auhors on the second column. Actually, it's up to you to
% strictly adhere to this author notation.
\author{%
  \alignauthor{%
    \textbf{First Author}\\
    \textbf{Second Author}\\
     \textbf{Third Author}\\
     \textbf{Fourth Author}\\
    \affaddr{University of Author} \\
    \affaddr{Authortown, CA 94022, USA} \\
    \email{author1@anotherco.edu} }\alignauthor{%
%    \textbf{Fifth Author}\\
%    \affaddr{YetAuthorCo, Inc.}\\
%    \affaddr{Authortown, BC V6M 22P Canada}\\
%    \email{author5@anotherco.com} } \vfil \alignauthor{%
%    \textbf{Second Author}\\
%    \affaddr{VP, Authoring}\\
%    \affaddr{Authorship Holdings, Ltd.}\\
%    \affaddr{Awdur SA22 8PP, UK}\\
%    \email{author2@author.ac.uk} }\alignauthor{%
%    \textbf{Sixth Author}\\
%    \affaddr{Universit\'e de Auteur-Sud}\\
%    \affaddr{40222 Auteur France}\\
%    \email{author6@author.fr} } \vfil \alignauthor{%
%    \textbf{Third Author}\\
%    \textbf{Fourth Author}\\    
%    \affaddr{L\={e}khaka Interaction Labs}\\
%    \affaddr{Bengaluru 560 080, India}\\
%    \email{author3@anotherco.com} \\
%    \email{author4@hchi.anotherco.com} }\alignauthor{%
%    \textbf{Seventh Author}\\
%    \affaddr{Department of Skrywer}\\
%    \affaddr{University of Umbhali}\\
%    \affaddr{Cape Town, South Africa}\\
%    \email{author7@umbhaliu.ac.za} 
} }

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}
% \reversemarginpar%

\begin{document}
%\SweaveOpts{concordance=TRUE}

%% For the camera ready, use the commands provided by the ACM in the Permission Release Form.
%\CopyrightYear{2007}
%\setcopyright{rightsretained}
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\isbn{0-12345-67-8/90/01}
%\doi{http://dx.doi.org/10.1145/2858036.2858119}
%% Then override the default copyright message with the \acmcopyright command.
%\copyrightinfo{\acmcopyright}

\maketitle

% Uncomment to disable hyphenation (not recommended)
% https://twitter.com/anjirokhan/status/546046683331973120
\RaggedRight{} 

% Do not change the page size or page settings.
%\begin{abstract}
%hello
%%Recognising, understanding and predicting human behaviour is a core goal of HCI. Video offers a rich source of information about behaviour, but coding this has traditionally been a laborious manual process. Here, we describe a system that can be trained to automatically recognise a particular behaviour. We report the results of an experiment examining the accuracy with which the system can determine the location of someone's visual attention, showing that training with frames picked at random produces significantly quicker and more accurate behaviour recognition than training with consecutive frames from the start of the video.
%\end{abstract}

\begin{abstract}
  %UPDATED---\today. This sample paper describes the formatting
  Recognising, understanding and predicting human behaviour is a core goal of HCI. Video offers a rich source of information about behaviour, but coding this has traditionally been a laborious manual process. Here we describe training a system to automatically recognise when a user switches attention from a TV to a tablet, comparing two approaches. The results show that training with frames picked at random produces significantly quicker and more accurate behaviour recognition than training with consecutive frames from the start of the video.
\end{abstract}


\keywords{\plainkeywords}

%\category{H.5.m}{Information interfaces and presentation (e.g.,
  %HCI)}{Miscellaneous}\category
  %{See}{\url{http://acm.org/about/class/1998/}}{for
%  full list of ACM classifiers. This section is required.}

\section{Introduction}

In a previous paper \cite{Apaolaza2016} we took showed how object tracking could be used to classify participants' behaviour. This can reduce the need for manual coding of behaviours in videos.  We used object tracking to follow participants movements throughout the entire video, and  used ground truth measurements from the initial part of the video to train a classifier. The classifier's predictions were then compared to the ground truth for the remainder of the video to evaluate its performance.  In this paper we show that an alternative approach to training the clasifier, where training frames are slected at random rather than sequentially, substantially improves the performance of the technique. We also compare the performance of object tracking (as used in \cite{Apaolaza2016}) to face detection.   We focus on reproducibility in our work; this paper is writen as a dynamic document in Knitr \cite{Knitr}, allowing others to verify, extend and modify our work.
<<setup, cache = FALSE, echo = FALSE, message=FALSE>>=
library(knitr)
library(tidyverse)
opts_chunk$set(cache = TRUE, autodep = TRUE)
copystate <- file.copy("RNGState.dontchange", "RNGstate", overwrite =  TRUE, copy.mode = FALSE)
if (copystate) {
  write("Copied RNGstate.dontchange to RNGstate for run", file = "Runlog.txt", append = FALSE)
} else {
  warning("Couldn't copy initial random state file")
}
@

<<numparts, engine='bash', echo = FALSE>>=
find ./part1GroundTruth/P??checkGTwebcam2.csv.gz -printf "%f\n" | cut -c1-3 > participantcodes.csv

@

<<echo = FALSE>>=
participantCode <- read.csv("participantcodes.csv", header = FALSE, stringsAsFactors = FALSE, 
                            col.names = "participantCode") 
numparticipants <- nrow(participantCode)
#unlink("participantcodes.csv")
@


<<usefulvalues, echo = FALSE>>=
fps <- 30

@


\section{Methods}

30 participants took part in a ``spot the difference'' experiment.  Each participant was given a tablet, positioned in front of a TV screen.   Their task was to spot the differences between a series of images shown simultaneously on the tablet and TV screen.  We captured front, side and top-down video, as shown in Figure \ref{fig:setup}.   We also positioned Microsoft Kinects in front of, and to the side of the participant.  All video sources recorded at \Sexpr{fps} frames per second. Participants also wore a Metawear movement sensors\cite{Metawear} on each wrist, which captured acceleration and angular rotation rates at $\sim 25$ fps.

\begin{marginfigure}%[-35pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/ExperimentSetup}
	  \caption{The experiment setup.  Only video from the front-facing (top left) camera was used in our analysis.\label{fig:setup}}
  \end{minipage}
\end{marginfigure}

\textit{We need a paragraph here on how the experiment worked - what happened in the training, part1 and part2 of it. How long each part of the experiment lasted.  This could be good to put in graphical time-line in the side bar??}

% We calculate the results before we discuss them, so that they exist in time for the results figure float
<<experimentVars, echo = FALSE>>=
seqtimes <- c(15,30,60)
randframes <- c(30, 60, 120, 240, 450)
experimentpart <- c("part1", "part2")
tracker <- c("openface", "cppMT")

seqframes <- seqtimes * fps
@

<<genconfigs, echo = FALSE>>=
participantCode <- participantCode
configurations <- tidyr::crossing(participantCode, experimentpart, tracker,
                                  bind_rows(tidyr::crossing(config = "shuffle", frames = randframes),
                                            tidyr::crossing(config = "noshuffle", frames = seqframes))
)
@


<<callClassifier, echo = FALSE>>=



getFrame <- function(participantCode, experimentpart, event){

  # Get the frame number that an experiemnt part start/ends at
  part <- stringr::str_match(experimentpart, "\\d+")  
  if (is.na(part)) {
    stop("Experiment part could not be extracted")
  }
  
  eventstring <- paste0(event, part)
  outputname <- tempfile()
  
  sysargs <- c(paste0("--attentionfile=", "./Attention/", participantCode, "_attention.txt"),
               paste0("--outputfile=", outputname),
               paste0("--participant=", participantCode),
               paste0("--event=", eventstring),
               paste0("--externaleventfile=./Attention/transitionAnnotations.csv")
               )
  
  system2("abc-display-tool/abc-extractAttention.py",
          args = sysargs,
          stdout = NULL
          )
  
  results <- read.csv(outputname)
  
  if (nrow(results) != 1) {
    stop("Error reading results frame")
  }
  
  outframe <- results[1, "frame"]
  
  unlink(outputname)
  
  return(outframe)
}


# It appears that the file is closed at the end of each Knitr chunk, so we reopen it here

logConn <- file("Runlog.txt", open = "at")
resultsframe <- NULL
for (i in 1:nrow(configurations)) {
  # Generating the results can take a long while, but anything we print will end up in the paper
  # So dump the current config to a file to allow progress to be monitored
  write(paste(configurations[i,], collapse = ":"), file = "Runlog.txt", append = TRUE)
  runoutput <- tempfile()

  if (configurations[i, "tracker"] == "openface") {
    trackerfn <- paste0(configurations[i, "participantCode"], "_front.openface.gz")
  } else if (configurations[i, "tracker"] == "cppMT") {
    trackerfn <- paste0(configurations[i, "participantCode"], "_",
                paste0(configurations[i,"experimentpart"], "_front_cppMT.csv.gz"))
  } else {  
    stop(paste(configurations[i, "tracker"], "filename formula not yet implemented"))
  }
  
  groundtruthpfn <- paste0("Groundtruth/", 
                           configurations[i, "participantCode"], "_groundtruth.csv")
  
  # Use abc-extractAttention to get start/end frame of each experiement
  startframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], "start")
  endframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], "end")
  
  
  runargs <- c(paste0("--trackerfile=", "Tracking/", trackerfn),
            paste0("--startframe=", startframe),
            paste0("--endframe=", endframe),
            paste0("--extgt=", groundtruthpfn),
            "--useexternalgt",
            paste0("--participantcode=", paste0(configurations[i,], collapse = ":")),
            paste0("--", configurations[i, "config"]),
            paste0("--summaryfile=", runoutput),
            paste0("--externaltrainingframes=", configurations[i,"frames"]),
            paste0("--rngstate=RNGstate")
            )
  
  system2("abc-display-tool/abc-classify.py",
          args = runargs,
          stdout = NULL
          )


  results <- read.csv(runoutput, header = FALSE,
                    col.names = c("configuration",
                                  "trainedframes",
                                  "startframe",
                                  "endframe",
                                  "crossvalAccuracy",
                                  "crossvalAccuracySD",
                                 "crossvalAccuracyLB",
                                 "crossvalAccuracyUB",
                                  "groundtruthAccuracy"
                                  ))
  if (nrow(results) > 1) {
    stop(paste("Something went wrong for ", configurations[i,]))
  }
  resultsframe <- rbind(resultsframe, 
                        data.frame(c(configurations[i,],  results)))
  unlink(runoutput)
}

@

<<echo=FALSE>>=
resultsframe <- as_tibble(resultsframe)

resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart) %>% 
                    summarise(avgxval = mean(crossvalAccuracy),
                    avggt = mean(groundtruthAccuracy), 
                    datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  stop("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
  resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)
accuracyfigure <- ggplot(data = resultsmean, aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_wrap( experimentpart ~ tracker, ncol = 1) + xlab("training frames") + 
  ylab("mean accuracy") + theme(legend.position = "bottom")
# We save the figure here - it gets pulled into the document later
ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")

@


For \Sexpr{numparticipants} of the participants\footnote{Work to encode the remaining participants is ongoing.}, the web-cam video was manually encoded \textit{using some software??}, to produce a ground-truth for each participant.  Participants' attention towards the tablet, TV and elsewhere was recorded (we combined the last two states in this analysis to consider only ``tablet'' and ``not tablet'' as possible behaviours). We recorded the start and end times of the transition between states. We encoded frames before the mid-point of each transition period to the original state, and the remaining frames to the new state. This data serves as our ground truth.

%We developed a bespoke tool to extract each frame of the colour video stream recorded by the Kinect in an eXtended Event File.  Occasional frames had been dropped by the Kinect when recording.  Since the ground truth behaviours were recorded against the web-cam time, we mapped "Kinect times" to "web-cam times" where required.

We took video from the front-facing webcam (recorded at 544x418 resolution)
%, and Kinect video (recorded at 1920x1080 resolution), 
and applied object tracking and face detection algorithms to both.   We used \texttt{OpenFace}\cite{Baltrusaitis2016} to track facial position, gaze and pose.  We ran \texttt{OpenFace} on the whole video for each participant, and then extracted the tracking data corresponding to each part of the experiment.  In common with our previous work, we used \texttt{CppMT}\cite{Nebehay2015CVPR} to perform object tracking on the video of each participant.  We had previously found that that object bounding box was prone to ``drift''; particularly if the participant's face became obscured (for example, by turning away from the camera).  For this reason, we (re)set the bounding box at the start of each part of the experiment.

We analysed parts 1 and 2 of the experiment separately.  We hypothesised that part 2 would prove more challenging for our approach; reviewing the videos we noted that participants would change the position of the tablet, and in some cases partially obscured their face with the tablet.

We developed a Python program to perform the classification process.  This could operate in a non-interactive mode, where an external ground truth file was used to train and evaluate the classifier.  When used in interactive mode, the user was presented with each frame in the video, which they could then classify to a numeric state.  An ``undo'' feature was provided.  We also allowed the user to review the frames at $\pm 3$ from the frame being classified.  We added this feature since it was sometimes difficult to determine the participant's behaviour if they were moving rapidly.  



\subsection{Evaluation of the classifier}
When the classifier is operating in non-interactive mode, we know the ground-truth state for each frame, which we can use to evaluate the classifier's performance.  When using the classifier ``for real'', in interactive mode, such data will not, in general be available. When training from a random sample of frames, we can exploit the randomness of the sample to estimate the classifier accuracy using a shuffle split cross validation approach.  This takes a randomly selected 50\% of the frames as hold-back, fits the classifier on the remaining frames, and calculates the accuracy. We report the mean of 1000 such evaluations. We cannot use this approach when classifying frames sequentially, since our training data are not a random sample of the video's frames. When running in interactive mode the user can also visually assess the classifier's performance by replaying the video, with a rectangle, whose colour indicates the predicted behaviour overlaid on each frame.   

% This is the results figure
\begin{marginfigure}[2pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/accuracy}
	  \caption{Mean accuracy of the sequential and randomised training processes. The dashed line shows in the mean accuracy estimated using the cross-validation procedure described in the main text.}\label{fig:resultsfig}
  \end{minipage}
\end{marginfigure}


The \texttt{DecisionTreeClassifier} in \texttt{sklearn} was used with its default parameters to classify the behaviour in each video frame.   We allowed all features identified by the tracking or facial recognition algorithm as possible covariates (with the exception of facial action units in the OpenFace data) in the decision tree.\footnote{We might want to say something about the risk of over-fitting here, and/or look at how similar/different the participants' trees are.} Two different approaches were used to train the classifier:
% Seqtimes and randframes are defined quite a long way up, since we've already calculated the results by this point
% so we can get the figure in the right place
\begin{enumerate}\compresslist
	\item Sequential training frames; $t \in \{ \Sexpr{seqtimes} \}$ seconds
	\item Random training frames; $f \in \{ \Sexpr{randframes} \}$ frames
\end{enumerate}
When training sequentially we used the ground truth data for the first $t$ seconds of each part of the experiment.  

\section{Results}
<<chooseplotpart, echo=FALSE>>=
#The experiment part and tracker to plot 
plotpart = "part1"
plottracker = "openface"

# Make these look nice for the main text
plotstring <- stringr::str_replace(plotpart, "(part)(\\d)", "\\1 \\2")
trackerstring <- ifelse(plottracker == "openface", "OpenFace", plottracker)
trackerstring <- ifelse(plottracker == "cppMT", "CppMT", plottracker)
@

For each participant and experiment part we trained the classifier for the specified number of frames using either the sequential or randomised training approach.  The accuracy of the classifer was evaluated the ground truth for all remaining frames.  In practice, ground truth data would be usually be unavailable to these frames; we also calculated, for the random sampling process, the classifier's estimated accuracy using the cross validation approach. Figure \ref{fig:participantFig} shows the results for each participant for \Sexpr{plotstring} of the experiment using \Sexpr{trackerstring}.  Figure \ref{fig:resultsfig} shows the average accuracy for each part of the experiment and each tracking method.  

<<participantFig, echo=FALSE, width='1.3\\columnwidth', fig.cap="Classification accuracy with randomised training frames for each participant.  The red line, and associated 95\\% confidence interval, indicate the estimated accuracy using the cross validation approach described in the main text.  The blue line shows the estimated accuracy using the ground truth for the remaining frames.">>=
participantplot <- ggplot(data = resultsframe[resultsframe$tracker == plottracker &
                                                resultsframe$experimentpart == plotpart &
                                                resultsframe$config == "shuffle",],
                          aes(x = frames, y = crossvalAccuracy, colour = config)) + 
  geom_ribbon(aes(ymin = crossvalAccuracyLB, ymax = crossvalAccuracyUB), alpha = 0.4, colour = NA) + 
  geom_line() + geom_line(aes(y = groundtruthAccuracy), colour = "blue") +
  facet_wrap(~ participantCode) + xlab("training frames") + ylab("accuracy") + theme(legend.position = "none") +
  ggtitle(paste("Participant accuracy,", plottracker, ",", plotpart)) 
print(participantplot)
@

\section{Reproducible research}
We use Docker images to run the CppMT and Openface on each of participant's video, as described in \cite{Apaolaza2016}.   We extend the idea of reproducibility to this paper, which is written using Knitr\cite{Knitr}.  This allows us to interleave R and \LaTeX~code.   For example, if we decide to evaluate a different number of training frames, we simply alter the values in our \texttt{randframes} array, and ``knit'' the document.  This calls the Python code to evaluate the performance of the classifier with the new values, and updates the graphs that depend on these values.  The Knitr source of this document, the ground truth files, and links the outputs CppMT and Openface\footnote{We are unable to make the source videos available owing to participant confidentiality.} are available at \url{https://github.com/IDInteraction/DIS2017}. From these it is possible to recreate this abstract, and vary the analysis parameters used.  

\section{Discussion \& Further Work}
 
 
We have shown that selecting training frames at random dramatically improves the performance of our approach.  When training the classifier with sequentially, the position of each feature is, in general, closely correlated with the previous frame.  As such each additional frame gives us relatively little new information.  Furthermore, the classifier is unlikely to be robust to changes in the participant's behaviour over the course of the experiment (for example if they change the position of the tablet) %\footnote{It would be interesting to see whether we get more complex (i.e.\ flexible/adaptable) decision trees with the random classifier}.
In contrast, by selecting training frames at random we will capture a representative sample of each participant's behaviour over the course of the experiment. 
 
Although it was straightforward to classify the majority of frames when working in interactive mode, frames where the participant was transitioning between states were sometimes difficult to classify.  We plan to do further work exploring how best to handle these frames; whether it is better to treat ``transition between states'' as a separate state, or whether we should consistently include transition frames in one or other of the states.  
 
We also plan to investigate whether including the accelerometer and gyroscope data in the classification tree improves prediction accuracy.   The Kinect data includes depth data.  It is, however, unclear how we might best reduce this to a form suitable for use in a decision tree classifier.  
 
Both methods of extracting scene features from the video frames gave an accuracy that was, on average, similar across all participants.  This masks some marked differences in performance for individual participants.  \texttt{OpenFace} is less general; it only detects faces, but has the advantage that it operates completely automatically, whereas a user specified bounding box is required for \texttt{CppMT}.  

\subsection{Acknowledgements}
We would like to thank the participants who took part in this experiment. We would like to thank the EPSRC and the BBC for their funding contribution to this work via grants EP/K503782/1, EP/H500154/1, and EP/M017133/1.\footnote{This is straight from \cite{Apaolaza2016} - not sure if it's the same grant codes?}

%\marginpar{
%  \vspace{-45pt} \fbox{
%    \begin{minipage}{0.925\marginparwidth}
%      \textbf{Good Utilization of the Side Bar} \\
%      \vspace{1pc} \textbf{Preparation:} Do not change the margin
%      dimensions and do not flow the margin text to the
%      next page. \\
%      \vspace{1pc} \textbf{Materials:} The margin box must not intrude
%      or overflow into the header or the footer, or the gutter space
%      between the margin paragraph and the main left column. The text
%      in this text box should remain the same size as the body
%      text. Use the \texttt{{\textbackslash}vspace{}} command to set
%      the margin
%      note's position. \\
%      \vspace{1pc} \textbf{Images \& Figures:} Practically anything
%      can be put in the margin if it fits. Use the
%      \texttt{{\textbackslash}marginparwidth} constant to set the
%      width of the figure, table, minipage, or whatever you are trying
%      to fit in this skinny space.
%    \end{minipage}}\label{sec:sidebar} }
%
%\section{Page Size}
%All SIGCHI submissions should be US letter (8.5 $\times$ 11
%inches). US Letter is the standard option used by this \LaTeX\
%template.
%
%\section{Text Formatting}
%Please use an 8.5-point Verdana font, or other sans serifs font as
%close as possible in appearance to Verdana in which these guidelines
%have been set. Arial 9-point font is a reasonable substitute for
%Verdana as it has a similar x-height. Please use serif or
%non-proportional fonts only for special purposes, such as
%distinguishing \texttt{source code} text.
%
%\subsubsection{Text styles}
%The \LaTeX\ template facilitates text formatting for normal (for body
%text); heading 1, heading 2, heading 3; bullet list; numbered list;
%caption; annotation (for notes in the narrow left margin); and
%references (for bibliographic entries). Additionally, here is an
%example of footnoted\footnote{Use footnotes sparingly, if at all.}
%text. As stated in the footnote, footnotes should rarely be used.
%
%\begin{figure}
%  \includegraphics[width=0.9\columnwidth]{figures/sigchi-logo}
%  \caption{Insert a caption below each figure.}~\label{fig:sample}
%\end{figure}
%
%\subsection{Language, style, and content}
%The written and spoken language of SIGCHI is English. Spelling and
%punctuation may use any dialect of English (e.g., British, Canadian,
%US, etc.) provided this is done consistently. Hyphenation is
%optional. To ensure suitability for an international audience, please
%pay attention to the following:
%
%\begin{table}
%  \centering
%  \begin{tabular}{l r r r}
%    % \toprule
%    & & \multicolumn{2}{c}{\small{\textbf{Test Conditions}}} \\
%    \cmidrule(r){3-4}
%    {\small\textit{Name}}
%    & {\small \textit{First}}
%      & {\small \textit{Second}}
%    & {\small \textit{Final}} \\
%    \midrule
%    Marsden & 223.0 & 44 & 432,321 \\
%    Nass & 22.2 & 16 & 234,333 \\
%    Borriello & 22.9 & 11 & 93,123 \\
%    Karat & 34.9 & 2200 & 103,322 \\
%    % \bottomrule
%  \end{tabular}
%  \caption{Table captions should be placed below the table. We
%    recommend table lines be 1 point, 25\% black. Minimize use of
%    table grid lines.}~\label{tab:table1}
%\end{table}
%
%\begin{itemize}\compresslist%
%\item Write in a straightforward style. Use simple sentence
%  structure. Try to avoid long sentences and complex sentence
%  structures. Use semicolons carefully.
%\item Use common and basic vocabulary (e.g., use the word ``unusual''
%  rather than the word ``arcane'').
%\item Briefly define or explain all technical terms. The terminology
%  common to your practice/discipline may be different in other design
%  practices/disciplines.
%\item Spell out all acronyms the first time they are used in your
%  text. For example, ``World Wide Web (WWW)''.
%\item Explain local references (e.g., not everyone knows all city
%  names in a particular country).
%\item Explain ``insider'' comments. Ensure that your whole audience
%  understands any reference whose meaning you do not describe (e.g.,
%  do not assume that everyone has used a Macintosh or a particular
%  application).
%\item Explain colloquial language and puns. Understanding phrases like
%  ``red herring'' requires a cultural knowledge of English. Humor and
%  irony are difficult to translate.
%\item Use unambiguous forms for culturally localized concepts, such as
%  times, dates, currencies, and numbers (e.g., ``1-5- 97'' or
%  ``5/1/97'' may mean 5 January or 1 May, and ``seven o'clock'' may
%  mean 7:00 am or 19:00). For currencies, indicate equivalences:
%  ``Participants were paid {\fontfamily{txr}\selectfont \textwon}
%  25,000, or roughly US \$22.''
%\item Be careful with the use of gender-specific pronouns (he, she)
%  and other gender-specific words (chairman, manpower,
%  man-months). Use inclusive language (e.g., she or he, they, chair,
%  staff, staff-hours, person-years) that is gender-neutral. If
%  necessary, you may be able to use ``he'' and ``she'' in alternating
%  sentences, so that the two genders occur equally
%  often~\cite{Schwartz:1995:GBF}.
%\item If possible, use the full (extended) alphabetic character set
%  for names of persons, institutions, and places (e.g.,
%  Gr{\o}nb{\ae}k, Lafreni\'ere, S\'anchez, Nguy{\~{\^{e}}}n,
%  Universit{\"a}t, Wei{\ss}enbach, Z{\"u}llighoven, \r{A}rhus, etc.).
%  These characters are already included in most versions and variants
%  of Times, Helvetica, and Arial fonts.
%\end{itemize}
%
% \begin{figure}
%   \includegraphics[width=.9\columnwidth]{figures/ea-figure2}
%   \caption{If your figure has a light background, you can set its
%     outline to light gray, like this, to make a box around
%     it.}\label{fig:bats}
% \end{figure}

%\begin{marginfigure}[-35pc]
%  \begin{minipage}{\marginparwidth}
%    \centering
%    \includegraphics[width=0.9\marginparwidth]{figures/cats}
%    \caption{In this image, the cats are tessellated within a square
%      frame. Images should also have captions and be within the
%      boundaries of the sidebar on page~\pageref{sec:sidebar}. Photo:
%      \cczero~jofish on Flickr.}~\label{fig:marginfig}
%  \end{minipage}
%\end{marginfigure}
%
%\section{Figures}
%The examples on this and following pages should help you get a feel
%for how screen-shots and other figures should be placed in the
%template. Your document may use color figures (see
%Figures~\ref{fig:sample}), which are included in the page limit; the
%figures must be usable when printed in black and white. You can use
%the \texttt{\marginpar} command to insert figures in the (left) margin
%of the document (see Figure~\ref{fig:marginfig}). Finally, be sure to
%make images large enough so the important details are legible and
%clear (see Figure~\ref{fig:cats}).
%
%\section{Tables}
%You man use tables inline with the text (see Table~\ref{tab:table1})
%or within the margin as shown in Table~\ref{tab:table2}. Try to
%minimize the use of lines (especially vertical lines). \LaTeX\ will
%set the table font and captions sizes correctly; the latter must
%remain unchanged.
%
%\section{Accessibility}
%The Executive Council of SIGCHI has committed to making SIGCHI
%conferences more inclusive for researchers, practitioners, and
%educators with disabilities. As a part of this goal, the all authors
%are asked to work on improving the accessibility of their
%submissions. Specifically, we encourage authors to carry out the
%following five steps:
%\begin{itemize}\compresslist%
%\item Add alternative text to all figures
%\item Mark table headings
%\item Generate a tagged PDF
%\item Verify the default language
%\item Set the tab order to ``Use Document Structure''
%\end{itemize}
%
%For links to instructions and resources, please see:
%\url{http://chi2016.acm.org/accessibility}
%
%Unfortunately good tools do not yet exist to create tagged PDF files
%from Latex. \LaTeX\ users will need to carry out all of the above
%steps in the PDF directly using Adobe Acrobat, after the PDF has been
%generated.
%
%For more information and links to instructions and resources, please
%see:
%\url{http://chi2016.acm.org/accessibility}.
%
%\begin{figure*}
%  \centering
%  \includegraphics[width=1.3\columnwidth]{figures/map}
%  \caption{In this image, the map maximizes use of space. You can make
%    figures as wide as you need, up to a maximum of the full width of
%    both columns. Note that \LaTeX\ tends to render large figures on a
%    dedicated page. Image: \ccbynd~ayman on Flickr.}~\label{fig:cats}
%\end{figure*}
%
%\section{Producing and Testing PDF Files}
%We recommend that you produce a PDF version of your submission well
%before the final deadline. Your PDF file must be ACM DL Compliant and
%meet stated requirements,
%\url{http://www.sheridanprinting.com/sigchi/ACM-SIG-distilling-settings.htm}.
%
%\marginpar{\vspace{-23pc}So long as you don't type outside the right
%  margin or bleed into the gutter, it's okay to put annotations over
%  here on the left, too; this annotation is near Hawaii. You'll have
%  to manually align the margin paragraphs to your \LaTeX\ floats using
%  the \texttt{{\textbackslash}vspace{}} command.}
%
%\begin{margintable}[1pc]
%  \begin{minipage}{\marginparwidth}
%    \centering
%    \begin{tabular}{r r l}
%      & {\small \textbf{First}}
%      & {\small \textbf{Location}} \\
%      \toprule
%      Child & 22.5 & Melbourne \\
%      Adult & 22.0 & Bogot\'a \\
%      \midrule
%      Gene & 22.0 & Palo Alto \\
%      John & 34.5 & Minneapolis \\
%      \bottomrule
%    \end{tabular}
%    \caption{A simple narrow table in the left margin
%      space.}~\label{tab:table2}
%  \end{minipage}
%\end{margintable}
%Test your PDF file by viewing or printing it with the same software we
%will use when we receive it, Adobe Acrobat Reader Version 10. This is
%widely available at no cost. Note that most
%reviewers will use a North American/European version of Acrobat
%reader, so please check your PDF accordingly.
%
%\section{Acknowledgements}
%We thank all the volunteers, publications support, staff, and authors
%who wrote and provided helpful comments on previous versions of this
%document. As well authors 1, 2, and 3 gratefully acknowledge the grant
%from NSF (\#1234--2222--ABC). Author 4 for example may want to
%acknowledge a supervisor/manager from their original employer. This
%whole paragraph is just for example. Some of the references cited in
%this paper are included for illustrative purposes only.
%
%\section{References Format}
%Your references should be published materials accessible to the
%public. Internal technical reports may be cited only if they are
%easily accessible and may be obtained by any reader for a nominal
%fee. Proprietary information may not be cited. Private communications
%should be acknowledged in the main text, not referenced (e.g.,
%[Golovchinsky, personal communication]). References must be the same
%font size as other body text. References should be in alphabetical
%order by last name of first author. Use a numbered list of references
%at the end of the article, ordered alphabetically by last name of
%first author, and referenced by numbers in brackets. For papers from
%conference proceedings, include the title of the paper and the name of
%the conference. Do not include the location of the conference or the
%exact date; do include the page numbers if available. 
%
%References should be in ACM citation format:
%\url{http://www.acm.org/publications/submissions/latex_style}.  This
%includes citations to Internet
%resources~\cite{CHINOSAUR:venue,cavender:writing,psy:gangnam}
%according to ACM format, although it is often appropriate to include
%URLs directly in the text, as above. Example reference formatting for
%individual journal articles~\cite{ethics}, articles in conference
%proceedings~\cite{Klemmer:2002:WSC:503376.503378},
%books~\cite{Schwartz:1995:GBF}, theses~\cite{sutherland:sketchpad},
%book chapters~\cite{winner:politics}, an entire journal
%issue~\cite{kaye:puc},
%websites~\cite{acm_categories,cavender:writing},
%tweets~\cite{CHINOSAUR:venue}, patents~\cite{heilig:sensorama}, 
%games~\cite{supermetroid:snes}, and
%online videos~\cite{psy:gangnam} is given here.  See the examples of
%citations at the end of this document and in the accompanying
%\texttt{BibTeX} document. This formatting is a edited version of the
%format automatically generated by the ACM Digital Library
%(\url{http://dl.acm.org}) as ``ACM Ref''. DOI and/or URL links are
%optional but encouraged as are full first names. Note that the
%Hyperlink style used throughout this document uses blue links;
%however, URLs in the references section may optionally appear in
%black.

\balance{} 

\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{DISbib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
