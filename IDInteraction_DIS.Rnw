\documentclass{sigchi-ext}
% Please be sure that you have the dependencies (i.e., additional
% LaTeX packages) to compile this example.
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[scaled=.92]{helvet} % for proper fonts
\usepackage{graphicx} % for EPS use the graphics package instead
\usepackage{balance}  % for useful for balancing the last columns
\usepackage{booktabs} % for pretty table rules
\usepackage{ccicons}  % for Creative Commons citation icons
\usepackage{ragged2e} % for tighter hyphenation

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{SIGCHI Extended Abstracts Sample File: Note Initial
  Caps} \def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Behavioural coding; object tracking; visual attention; reproducible methods}
\def\plaingeneralterms{Documentation, Standardization}

\title{How should we train a tool to recognise behaviour?}

\numberofauthors{6}
% Notice how author names are alternately typesetted to appear ordered
% in 2-column format; i.e., the first 4 autors on the first column and
% the other 4 auhors on the second column. Actually, it's up to you to
% strictly adhere to this author notation.
\author{%
  \alignauthor{%
    \textbf{First Author}\\
    \textbf{Second Author}\\
     \textbf{Third Author}\\
     \textbf{Fourth Author}\\
    \affaddr{University of Author} \\
    \affaddr{Authortown, CA 94022, USA} \\
    \email{author1@anotherco.edu} }\alignauthor{%
%    \textbf{Fifth Author}\\
%    \affaddr{YetAuthorCo, Inc.}\\
%    \affaddr{Authortown, BC V6M 22P Canada}\\
%    \email{author5@anotherco.com} } \vfil \alignauthor{%
%    \textbf{Second Author}\\
%    \affaddr{VP, Authoring}\\
%    \affaddr{Authorship Holdings, Ltd.}\\
%    \affaddr{Awdur SA22 8PP, UK}\\
%    \email{author2@author.ac.uk} }\alignauthor{%
%    \textbf{Sixth Author}\\
%    \affaddr{Universit\'e de Auteur-Sud}\\
%    \affaddr{40222 Auteur France}\\
%    \email{author6@author.fr} } \vfil \alignauthor{%
%    \textbf{Third Author}\\
%    \textbf{Fourth Author}\\    
%    \affaddr{L\={e}khaka Interaction Labs}\\
%    \affaddr{Bengaluru 560 080, India}\\
%    \email{author3@anotherco.com} \\
%    \email{author4@hchi.anotherco.com} }\alignauthor{%
%    \textbf{Seventh Author}\\
%    \affaddr{Department of Skrywer}\\
%    \affaddr{University of Umbhali}\\
%    \affaddr{Cape Town, South Africa}\\
%    \email{author7@umbhaliu.ac.za} 
} }

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}
% \reversemarginpar%

\begin{document}

%% For the camera ready, use the commands provided by the ACM in the Permission Release Form.
%\CopyrightYear{2007}
%\setcopyright{rightsretained}
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\isbn{0-12345-67-8/90/01}
%\doi{http://dx.doi.org/10.1145/2858036.2858119}
%% Then override the default copyright message with the \acmcopyright command.
%\copyrightinfo{\acmcopyright}

\maketitle

% Uncomment to disable hyphenation (not recommended)
% https://twitter.com/anjirokhan/status/546046683331973120
\RaggedRight{} 

\begin{abstract}
  %UPDATED---\today. This sample paper describes the formatting
  Recognising, understanding and predicting human behaviour is a core goal of HCI. Video offers a rich source of information about behaviour, but coding this has traditionally been a laborious manual process. Here we describe training a system to automatically recognise when a user switches attention from a TV to a tablet, comparing two approaches. The results show that training with frames picked at random produces significantly quicker and more accurate behaviour recognition than training with consecutive frames from the start of the video.
\end{abstract}


\keywords{\plainkeywords}

%\category{H.5.m}{Information interfaces and presentation (e.g.,
  %HCI)}{Miscellaneous}\category
  %{See}{\url{http://acm.org/about/class/1998/}}{for
%  full list of ACM classifiers. This section is required.}

\section{Introduction}

In a previous paper \cite{Apaolaza2016} we took showed how object tracking could be used to classify participants' behaviour. This can reduce the need for manual coding of behaviours in videos.  We used object tracking to follow participants movements throughout the entire video, and  used ground truth measurements from the initial part of the video to train a classifier. The classifier's predictions were then compared to the ground truth for the remainder of the video to evaluate its performance.  In this paper we show that an alternative approach to training the clasifier, where training frames are selected at random, rather than sequentially, substantially improves the performance of the technique. We also compare the performance of object tracking (as used in \cite{Apaolaza2016}) to face detection.   We focus on reproducibility in our work; this paper is writen as a dynamic document in Knitr \cite{Knitr}, allowing others to verify, extend and modify our work.
<<setup, cache = FALSE, echo = FALSE, message=FALSE>>=
library(knitr)
library(tidyverse)
opts_chunk$set(cache = TRUE, autodep = TRUE)
copystate <- file.copy("RNGState.dontchange", "RNGstate", overwrite =  TRUE, copy.mode = FALSE)
if (copystate) {
  write("Copied RNGstate.dontchange to RNGstate for run", file = "Runlog.txt", append = FALSE)
} else {
  warning("Couldn't copy initial random state file")
}
@



<<echo=FALSE>>=
unlink("participantcodes.csv")
system2("./writeParticipantCodes.sh")

@


<<echo = FALSE>>=
participantCode <- read.csv("participantcodes.csv", header = FALSE, stringsAsFactors = FALSE, 
                            col.names = "participantCode") 
numparticipants <- nrow(participantCode)
#unlink("participantcodes.csv")

if (numparticipants == 0 ) {
  "Stop - no participants found.  Check groundtruth folder contains data"
  
}
@


<<usefulvalues, echo = FALSE>>=
fps <- 30

@


%\begin{marginfigure}
%\end{marginfigure}

\section{Methods}
<<getframe, echo = FALSE>>=

getFrame <- function(participantCode, experimentpart, event){

  # Get the frame number that an experiemnt part start/ends at
  part <- stringr::str_match(experimentpart, "\\d+")  
  if (is.na(part)) {
    stop("Experiment part could not be extracted")
  }
  
  eventstring <- paste0(event, part)
  outputname <- tempfile()
  
  sysargs <- c(paste0("--attentionfile=", "./Attention/", participantCode, "_attention.txt"),
               paste0("--outputfile=", outputname),
               paste0("--participant=", participantCode),
               paste0("--event=", eventstring),
               paste0("--externaleventfile=./Attention/transitionAnnotations.csv")
               )
  
  system2("abc-display-tool/abc-extractAttention.py",
          args = sysargs,
          stdout = NULL
          )
  
  results <- read.csv(outputname)
  
  if (nrow(results) != 1) {
    stop("Error reading results frame")
  }
  
  outframe <- results[1, "frame"]
  
  unlink(outputname)
  
  return(outframe)
}

@


<<getlengths, echo = FALSE>>=
# Calculate the length of each experiment part for each participant
experimentlengths <- NULL
for (p in participantCode$participantCode) {
  for (part in c(1,2)) {
    partstring <- paste0("part", part)
    partlength  <-   getFrame(p, partstring, "end") - getFrame(p, partstring, "start")
    thisrow <- data.frame(participantCode = p, 
                          part = part,
                          length = partlength/fps)
    
    experimentlengths <- rbind(experimentlengths, thisrow)
  } 
  
}
experimentlengths$part <- as.factor(experimentlengths$part)
@

<<echo=FALSE>>=
exptTimeFig <- ggplot(data = experimentlengths, aes(part, length)) + geom_boxplot() + xlab("Experiment part") +
  ylab("Experiment length (seconds)")

ggsave("figures/exptTime.pdf", plot = exptTimeFig, device = "pdf", width = 8, height = 10, units = "cm")
@


\begin{marginfigure}[5pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/ExperimentSetup}
	  \caption{The experiment setup.  Only video from the front-facing (top left) camera was used in our analysis.\label{fig:setup}}
  \end{minipage}
\vskip 2cm
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/exptTime}
	  \caption{Time distribution of each part of the experiment. \label{fig:exptTime}}
  \end{minipage}
\end{marginfigure}

30 participants took part in a ``spot the difference'' experiment.  Each participant was given a tablet, positioned in front of a TV screen.   Their task was to spot the differences between a series of images shown simultaneously on the tablet and TV screen.  We captured front, side and top-down video, as shown in Figure \ref{fig:setup}.   We also positioned Microsoft Kinects in front of, and to the side of the participant.  All video sources recorded at \Sexpr{fps} frames per second. Participants also wore a Metawear movement sensors\cite{Metawear} on each wrist, which captured acceleration and angular rotation rates at $\sim 25$ fps.

The experiment started with an accommodation interval, followed by the main observation separated in two parts. Throughout the experiment, the participants carried out the same task of spotting differences between the image shown in the table and the one shown on the TV. During the first part of the main observation, participants were instructed to leave the tablet fixed on the table during the execution of the task. During the accommodation interval and the second part, participants were free to either pick up the tablet or leave it on the table.

The accommodation interval lasted for a minute, and the captured data was discarded from the analysis.
The remaining two parts lasted for two minutes and a half each. The interruptions were not strict. Once the time limit was reached, the system would wait for the participant to request the next image to indicate the end of that segment. Figure \ref{fig:exptTime} shows the time distributtion of each experiment part. Occassionally the experimenter would instruct the participants to request the next image, to prevent unusually long observations.

% We calculate the results before we discuss them, so that they exist in time for the results figure float
<<experimentVars, echo = FALSE>>=
seqtimes <- c(15,30,60)
randframes <- c(30, 60, 120, 240, 450)
experimentpart <- c("part1", "part2")
tracker <- c("openface", "cppMT")

seqframes <- seqtimes * fps
@

<<genconfigs, echo = FALSE>>=
participantCode <- participantCode
configurations <- tidyr::crossing(participantCode, experimentpart, tracker,
                                  bind_rows(tidyr::crossing(config = "shuffle", frames = randframes),
                                            tidyr::crossing(config = "noshuffle", frames = seqframes))
)
@


<<callClassifier, echo = FALSE>>=




# It appears that the file is closed at the end of each Knitr chunk, so we reopen it here

logConn <- file("Runlog.txt", open = "at")
resultsframe <- NULL
for (i in 1:nrow(configurations)) {
  # Generating the results can take a long while, but anything we print will end up in the paper
  # So dump the current config to a file to allow progress to be monitored
  write(paste(configurations[i,], collapse = ":"), file = "Runlog.txt", append = TRUE)
  runoutput <- tempfile()

  if (configurations[i, "tracker"] == "openface") {
    trackerfn <- paste0(configurations[i, "participantCode"], "_front.openface.gz")
  } else if (configurations[i, "tracker"] == "cppMT") {
    trackerfn <- paste0(configurations[i, "participantCode"], "_",
                paste0(configurations[i,"experimentpart"], "_front_cppMT.csv.gz"))
  } else {  
    stop(paste(configurations[i, "tracker"], "filename formula not yet implemented"))
  }
  
  groundtruthpfn <- paste0("Groundtruth/", 
                           configurations[i, "participantCode"], "_groundtruth.csv")
  
  # Use abc-extractAttention to get start/end frame of each experiement
  startframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], "start")
  endframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], "end")
  
  
  runargs <- c(paste0("--trackerfile=", "Tracking/", trackerfn),
            paste0("--startframe=", startframe),
            paste0("--endframe=", endframe),
            paste0("--extgt=", groundtruthpfn),
            "--useexternalgt",
            paste0("--participantcode=", paste0(configurations[i,], collapse = ":")),
            paste0("--", configurations[i, "config"]),
            paste0("--summaryfile=", runoutput),
            paste0("--externaltrainingframes=", configurations[i,"frames"]),
            paste0("--rngstate=RNGstate")
            )
  
  system2("abc-display-tool/abc-classify.py",
          args = runargs,
          stdout = NULL
          )


  results <- read.csv(runoutput, header = FALSE,
                    col.names = c("configuration",
                                  "trainedframes",
                                  "startframe",
                                  "endframe",
                                  "crossvalAccuracy",
                                  "crossvalAccuracySD",
                                 "crossvalAccuracyLB",
                                 "crossvalAccuracyUB",
                                  "groundtruthAccuracy"
                                  ))
  if (nrow(results) > 1) {
    stop(paste("Something went wrong for ", configurations[i,]))
  }
  resultsframe <- rbind(resultsframe, 
                        data.frame(c(configurations[i,],  results)))
  unlink(runoutput)
}

@

<<echo=FALSE>>=
resultsframe <- as_tibble(resultsframe)
write.csv(resultsframe, file=paste0("results", Sys.Date(), ".csv"))
resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart) %>% 
                    summarise(avgxval = mean(crossvalAccuracy),
                    avggt = mean(groundtruthAccuracy), 
                    datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  stop("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
  resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)
accuracyfigure <- ggplot(data = resultsmean, aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_wrap( experimentpart ~ tracker, ncol = 1) + xlab("training frames") + 
  ylab("mean accuracy") + theme(legend.position = "bottom")
# We save the figure here - it gets pulled into the document later
ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")

@


For \Sexpr{numparticipants} of the participants\footnote{Work to encode the remaining participants is ongoing.}, the web-cam video was manually encoded using ELAN\cite{ELAN}%,  software to annotate video and audio resources, 
to produce a ground-truth for each participant. For each participant, each change in attention was annotated, taking into account the transition period between the attention foci. Participants' attention towards the tablet, TV and elsewhere was recorded (we combined the last two states in this analysis to consider only ``tablet'' and ``not tablet'' as possible behaviours).  We encoded frames before the mid-point of each transition period to the original state, and the remaining frames to the new state. This data serves as our ground truth.


We took video from the front-facing webcam (recorded at 544x418 resolution)
and applied object tracking and face detection algorithms to it.   We used \texttt{OpenFace}\cite{Baltrusaitis2016} to track facial position, gaze and pose.  We ran \texttt{OpenFace} on the whole video for each participant, and then extracted the tracking data corresponding to each part of the experiment.  In common with our previous work, we used \texttt{CppMT}\cite{Nebehay2015CVPR} to perform object tracking on the video of each participant.  We had previously found that that object bounding box could be prone to ``drift''; particularly if the participant's face became obscured (for example, by turning away from the camera).  For this reason, we (re)set the bounding box at the start of each part of the experiment.

We analysed parts 1 and 2 of the experiment separately.  We hypothesised that part 2 would prove more challenging for our approach; reviewing the videos we noted that participants would change the position of the tablet, and in some cases partially obscured their face with the tablet.

% This is the results figure
\begin{marginfigure}[2pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/accuracy}
	  \caption{Mean accuracy of the sequential and randomised training processes. The dashed line shows in the mean accuracy estimated using the cross-validation procedure described in the main text.}\label{fig:resultsfig}
  \end{minipage}
\end{marginfigure}

We developed a Python program to perform the classification process.  This could operate in a non-interactive mode, where an external ground truth file was used to train and evaluate the classifier.  When used in interactive mode, the user was presented with each frame in the video, which they could then classify to a numeric state.  An ``undo'' feature was provided.  We also allowed the user to review the frames at $\pm 3$ from the frame being classified.  We added this feature since it was sometimes difficult to determine the participant's behaviour if they were moving rapidly.  



\subsection{Evaluation of the classifier}
When the classifier is operating in non-interactive mode, we know the ground-truth state for each frame, which we can use to evaluate the classifier's performance.  When using the classifier ``for real'', in interactive mode, such data will not, in general be available. When training from a random sample of frames, we can exploit the randomness of the sample to estimate the classifier accuracy using a shuffle split cross validation approach.  This takes a randomly selected 50\% of the frames as hold-back, fits the classifier on the remaining frames, and calculates the accuracy. We report the mean of 1000 such evaluations. We cannot use this approach when classifying frames sequentially, since our training data are not a random sample of the video's frames. When running in interactive mode the user can also visually assess the classifier's performance by replaying the video, with a rectangle, whose colour indicates the predicted behaviour, overlaid on each frame.   



The \texttt{DecisionTreeClassifier} in Python's \texttt{sklearn}\cite{scikit-learn} was used with its default parameters to classify the behaviour in each video frame.   We allowed all features identified by the tracking or facial recognition algorithm as possible covariates (with the exception of facial action units in the OpenFace data) in the decision tree.\footnote{We might want to say something about the risk of over-fitting here, and/or look at how similar/different the participants' trees are.} Two different approaches were used to train the classifier:
% Seqtimes and randframes are defined quite a long way up, since we've already calculated the results by this point
% so we can get the figure in the right place
\begin{enumerate}\compresslist
	\item Sequential training frames; $t \in \{ \Sexpr{seqtimes} \}$ seconds
	\item Random training frames; $f \in \{ \Sexpr{randframes} \}$ frames
\end{enumerate}
When training sequentially we used the ground truth data for the first $t$ seconds of each part of the experiment.  

\section{Results}
<<chooseplotpart, echo=FALSE>>=
#The experiment part and tracker to plot 
plotpart = "part1"
plottracker = "openface"

# Make these look nice for the main text
plotstring <- stringr::str_replace(plotpart, "(part)(\\d)", "\\1 \\2")
trackerstring <- ifelse(plottracker == "openface", "OpenFace", plottracker)
trackerstring <- ifelse(plottracker == "cppMT", "CppMT", plottracker)
@

For each participant and experiment part we trained the classifier for the specified number of frames using either the sequential or randomised training approach.  The accuracy of the classifer was evaluated the ground truth for all remaining frames. Figure \ref{fig:resultsfig} shows the average accuracy for each part of the experiment and each tracking method. In practice, ground truth data would be usually be unavailable for frames that had not yet been classified.  We also calculated, for the random sampling process, the classifier's estimated accuracy using the cross validation approach. Figure \ref{fig:participantFig} shows the results for each participant for \Sexpr{plotstring} of the experiment using \Sexpr{trackerstring}.    

<<participantFig, echo=FALSE, width='1.3\\columnwidth', fig.cap="Classification accuracy with randomised training frames for each participant.  The red line, and associated 95\\% confidence interval, indicate the estimated accuracy using the cross validation approach described in the main text.  The blue line shows the estimated accuracy using the ground truth for the remaining frames.">>=
participantplot <- ggplot(data = resultsframe[resultsframe$tracker == plottracker &
                                                resultsframe$experimentpart == plotpart &
                                                resultsframe$config == "shuffle",],
                          aes(x = frames, y = crossvalAccuracy, colour = config)) + 
  geom_ribbon(aes(ymin = crossvalAccuracyLB, ymax = crossvalAccuracyUB), alpha = 0.4, colour = NA) + 
  geom_line() + geom_line(aes(y = groundtruthAccuracy), colour = "blue") +
  facet_wrap(~ participantCode) + xlab("training frames") + ylab("accuracy") + theme(legend.position = "none") +
  ggtitle(paste("Participant accuracy,", plottracker, ",", plotpart)) 
print(participantplot)
@

\section{Reproducible research}
We use Docker images to run the CppMT and Openface on each of participant's video, as described in \cite{Apaolaza2016}.   We extend the idea of reproducibility to this paper, which is written using Knitr\cite{Knitr}.  This allows us to interleave R and \LaTeX~code.   For example, if we decide to evaluate a different number of training frames, we simply alter the values in our \texttt{randframes} array, and ``knit'' the document.  This calls the Python code to evaluate the performance of the classifier with the new values, and updates the graphs that depend on these values.  The Knitr source of this document, the ground truth files, and links to the outputs from CppMT and Openface\footnote{We are unable to make the source videos available owing to participant confidentiality.} are available at \url{https://github.com/IDInteraction/DIS2017}. From these it is possible to recreate this abstract, and vary the analysis parameters used.  

\section{Discussion \& Further Work}
 
 
We have shown that selecting training frames at random dramatically improves the performance of our approach.  When training the classifier with sequentially, the position of each feature is, in general, closely correlated with the previous frame.  As such each additional frame gives us relatively little new information.  Furthermore, the classifier is unlikely to be robust to changes in the participant's behaviour over the course of the experiment (for example if they change the position of the tablet) %\footnote{It would be interesting to see whether we get more complex (i.e.\ flexible/adaptable) decision trees with the random classifier}.
In contrast, by selecting training frames at random we will capture a representative sample of each participant's behaviour over the course of the experiment. 
 
Although it was straightforward to classify the majority of frames when working in interactive mode, frames where the participant was transitioning between states were sometimes difficult to classify.  We plan to do further work exploring how best to handle these frames; whether it is better to treat ``transition between states'' as a separate state, or whether we should consistently include transition frames in one or other of the states.  

 Both methods of extracting scene features from the video frames gave an accuracy that was, on average, similar across all participants.  This masks some marked differences in performance for individual participants\footnote{We haven't shown this in the paper\ldots perhaps show just some participants, and the different trackers/parts in figure \ref{fig:participantFig}?}.  \texttt{OpenFace} is less general; it only detects faces, but has the advantage that it operates completely automatically, whereas a user specified bounding box is required for \texttt{CppMT}.  

We also plan to investigate using the remaining sensor data (e.g.\ the sensor and Kinect data) to improve the accuracy of our predictions.  We note that the ``tracking'' and ``classification'' parts of our approach are distinct, and that other, domain specifc, ``tracking'' approaches may also be well suited to generate data for the classifer.
 


\subsection{Acknowledgements}
We would like to thank the participants who took part in this experiment. We would like to thank the EPSRC and the BBC for their funding contribution to this work via grants EP/K503782/1, EP/H500154/1, and EP/M017133/1.\footnote{This is straight from \cite{Apaolaza2016} - not sure if it's the same grant codes?}

\balance{} 

\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{DISbib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
